@incollection{hardleCanonicalCorrelationAnalysis2015,
  title = {Canonical {{Correlation Analysis}}},
  booktitle = {Applied {{Multivariate Statistical Analysis}}},
  author = {H{\"a}rdle, Wolfgang Karl and Simar, L{\'e}opold},
  editor = {H{\"a}rdle, Wolfgang Karl and Simar, L{\'e}opold},
  year = {2015},
  pages = {443--454},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-45171-7_16},
  abstract = {Complex multivariate data structures are better understood by studying low-dimensional projections. For a joint study of two data sets, we may ask what type of low-dimensional projection helps in finding possible joint structures for the two samples. The canonical correlation analysis (CCA) is a standard tool of multivariate statistical analysis for discovery and quantification of associations between two sets of variables.},
  isbn = {978-3-662-45171-7},
  keywords = {Canonical Correlation,Canonical Correlation Analysis,Canonical Variable,Contingency Table,Multivariate Statistical Analysis},
  language = {en}
}

@article{hardoonCanonicalCorrelationAnalysis2004,
  title = {Canonical {{Correlation Analysis}}: {{An Overview}} with {{Application}} to {{Learning Methods}}},
  shorttitle = {Canonical {{Correlation Analysis}}},
  author = {Hardoon, David R. and Szedmak, Sandor and {Shawe-Taylor}, John},
  year = {2004},
  month = dec,
  volume = {16},
  pages = {2639--2664},
  issn = {0899-7667},
  doi = {10.1162/0899766042321814},
  abstract = {We present a general method using kernel canonical correlation analysis to learn a semantic representation to web images and their associated text. The semantic space provides a common representation and enables a comparison between the text and images. In the experiments, we look at two approaches of retrieving images based on only their content from a text query. We compare orthogonalization approaches against a standard cross-representation retrieval technique known as the generalized vector space model.},
  journal = {Neural Computation},
  number = {12}
}

./.

@article{knappCanonicalCorrelationAnalysis1978,
  title = {Canonical Correlation Analysis: {{A}} General Parametric Significance-Testing System},
  shorttitle = {Canonical Correlation Analysis},
  author = {Knapp, Thomas R.},
  year = {1978},
  volume = {85},
  pages = {410--416},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.85.2.410},
  abstract = {Suggests that significance tests for 9 of the most common statistical procedures (simple correlation, t test for independent samples, multiple regression analysis, 1-way ANOVA, factorial ANOVA, analysis of covariance, t test for correlated samples, discriminant analysis, and chi-square test of independence) can all be treated as special cases of the test of the null hypothesis in canonical correlation analysis for 2 sets of variables. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Bulletin},
  keywords = {Statistical Analysis,Statistical Correlation,Statistical Significance,Statistical Tests},
  number = {2}
}


@article{chaudhuriFastAlgorithmComputing2019,
  title = {A Fast Algorithm for Computing Distance Correlation},
  author = {Chaudhuri, Arin and Hu, Wenhao},
  year = {2019},
  month = jul,
  volume = {135},
  pages = {15--24},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2019.01.016},
  abstract = {Classical dependence measures such as Pearson correlation, Spearman's {$\rho$}, and Kendall's {$\tau$} can detect only monotonic or linear dependence. To overcome these limitations, Sz\'ekely et al. proposed distance covariance and its derived correlation. The distance covariance is a weighted L2 distance between the joint characteristic function and the product of marginal distributions; it is 0 if and only if two random vectors X and Y are independent. This measure can detect the presence of a dependence structure when the sample size is large enough. They further showed that the sample distance covariance can be calculated simply from modified Euclidean distances, which typically requires O(n2) cost, where n is the sample size. Quadratic computing time greatly limits the use of the distance covariance for large data. To calculate the sample distance covariance between two univariate random variables, a simple, exact O(nlog(n)) algorithms is developed. The proposed algorithm essentially consists of two sorting steps, so it is easy to implement. Empirical results show that the proposed algorithm is significantly faster than state-of-the-art methods. The algorithm's speed will enable researchers to explore complicated dependence structures in large datasets.},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {Dependency measure,Distance correlation,Fast algorithm,Merge sort},
  language = {en}
}

@article{szekelyMeasuringTestingDependence2007,
  title = {Measuring and Testing Dependence by Correlation of Distances},
  author = {Sz{\'e}kely, G{\'a}bor J. and Rizzo, Maria L. and Bakirov, Nail K.},
  year = {2007},
  month = dec,
  volume = {35},
  pages = {2769--2794},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000505},
  abstract = {Distance correlation is a new measure of dependence between random vectors. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but unlike the classical definition of correlation, distance correlation is zero only if the random vectors are independent. The empirical distance dependence measures are based on certain Euclidean distances between sample elements rather than sample moments, yet have a compact representation analogous to the classical covariance and correlation. Asymptotic properties and applications in testing independence are discussed. Implementation of the test and Monte Carlo results are also presented.},
  journal = {The Annals of Statistics},
  keywords = {62G10,62H20,Distance correlation,distance covariance,multivariate independence},
  number = {6}
}

@article{szekelyPartialDistanceCorrelation2014a,
  title = {Partial Distance Correlation with Methods for Dissimilarities},
  author = {Sz{\'e}kely, G{\'a}bor J. and Rizzo, Maria L.},
  year = {2014},
  month = dec,
  volume = {42},
  pages = {2382--2412},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/14-AOS1255},
  abstract = {Distance covariance and distance correlation are scalar coefficients that characterize independence of random vectors in arbitrary dimension. Properties, extensions and applications of distance correlation have been discussed in the recent literature, but the problem of defining the partial distance correlation has remained an open question of considerable interest. The problem of partial distance correlation is more complex than partial correlation partly because the squared distance covariance is not an inner product in the usual linear space. For the definition of partial distance correlation, we introduce a new Hilbert space where the squared distance covariance is the inner product. We define the partial distance correlation statistics with the help of this Hilbert space, and develop and implement a test for zero partial distance correlation. Our intermediate results provide an unbiased estimator of squared distance covariance, and a neat solution to the problem of distance correlation for dissimilarities rather than distances.},
  journal = {The Annals of Statistics},
  keywords = {62Gxx,62H15,62H20,62Hxx,dissimilarity,energy statistics,independence,multivariate,partial distance correlation},
  number = {6}
}

@article{hellerConsistentMultivariateTest2013,
  title = {A Consistent Multivariate Test of Association Based on Ranks of Distances},
  author = {Heller, Ruth and Heller, Yair and Gorfine, Malka},
  year = {2013},
  volume = {100},
  pages = {503--510},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  abstract = {We consider the problem of detecting associations between random vectors of any dimension. Few tests of independence exist that are consistent against all dependent alternatives. We propose a powerful test that is applicable in all dimensions and consistent against all alternatives. The test has a simple form, is easy to implement, and has good power.},
  journal = {Biometrika},
  number = {2}
}

@article{grettonConsistentNonparametricTests2010,
  title = {Consistent {{Nonparametric Tests}} of {{Independence}}},
  author = {Gretton, Arthur and Gy{\"o}rfi, L{\'a}szl{\'o}},
  year = {2010},
  volume = {11},
  pages = {1391--1423},
  issn = {1533-7928},
  abstract = {Three simple and explicit procedures for testing the independence of two multi-dimensional random variables are described. Two of the associated test statistics (L1, log-likelihood) are defined when the empirical distribution of the variables is restricted to finite partitions. A third test statistic is defined as a kernel-based independence measure. Two kinds of tests are provided. Distribution-free strong consistent tests are derived on the basis of large deviation bounds on the test statistics: these tests make almost surely no Type I or Type II error after a random sample size. Asymptotically {$\alpha$}-level tests are obtained from the limiting distribution of the test statistics. For the latter tests, the Type I error converges to a fixed non-zero value {$\alpha$}, and the Type II error drops to zero, for increasing sample size. All tests reject the null hypothesis of independence if the test statistics become large. The performance of the tests is evaluated experimentally on benchmark data.},
  journal = {Journal of Machine Learning Research},
  number = {46}
}

@article{grettonKernelStatisticalTest2007,
  title = {A {{Kernel Statistical Test}} of {{Independence}}},
  author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon and Song, Le and Sch{\"o}lkopf, Bernhard and Smola, Alex},
  year = {2007},
  volume = {20},
  journal = {Advances in Neural Information Processing Systems},
  language = {en}
}

@article{sejdinovicEquivalenceDistancebasedRKHSbased2013,
  title = {Equivalence of Distance-Based and {{RKHS}}-Based Statistics in Hypothesis Testing},
  author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
  year = {2013},
  month = oct,
  volume = {41},
  pages = {2263--2291},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/13-AOS1140},
  abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
  journal = {The Annals of Statistics},
  keywords = {46E22,62G10,62H20,68Q32,distance covariance,Independence testing,reproducing kernel Hilbert spaces,two-sample testing},
  number = {5}
}

@article{shenExactEquivalenceDistance2020,
  title = {The Exact Equivalence of Distance and Kernel Methods in Hypothesis Testing},
  author = {Shen, Cencheng and Vogelstein, Joshua T.},
  year = {2020},
  month = sep,
  issn = {1863-818X},
  doi = {10.1007/s10182-020-00378-1},
  abstract = {Distance correlation and Hilbert-Schmidt independence criterion are widely used for independence testing, two-sample testing, and many inference tasks in statistics and machine learning. These two methods are tightly related, yet are treated as two different entities in the majority of existing literature. In this paper, we propose a simple and elegant bijection between metric and kernel. The bijective transformation better preserves the similarity structure, allows distance correlation and Hilbert-Schmidt independence criterion to be always the same for hypothesis testing, streamlines the code base for implementation, and enables a rich literature of distance-based and kernel-based methodologies to directly communicate with each other.},
  journal = {AStA Advances in Statistical Analysis},
  language = {en}
}

@article{shenLearningInterpretableCharacteristic2020,
  title = {Learning {{Interpretable Characteristic Kernels}} via {{Decision Forests}}},
  author = {Shen, Cencheng and Panda, Sambit and Vogelstein, Joshua T.},
  year = {2020},
  month = sep,
  abstract = {Decision forests are popular tools for classification and regression. These forests naturally produce proximity matrices measuring how often each pair of observations lies in the same leaf node. It has been demonstrated that these proximity matrices can be thought of as kernels, connecting the decision forest literature to the extensive kernel machine literature. While other kernels are known to have strong theoretical properties such as being characteristic, no similar result is available for any decision forest based kernel. In this manuscript,we prove that the decision forest induced proximity can be made characteristic, which can be used to yield a universally consistent statistic for testing independence. We demonstrate the performance of the induced kernel on a suite of 20 high-dimensional independence test settings. We also show how this learning kernel offers insights into relative feature importance. The decision forest induced kernel typically achieves substantially higher testing power than existing popular methods in statistical tests.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1812.00029},
  eprinttype = {arxiv},
  journal = {arXiv:1812.00029 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{shenHighDimensionalIndependenceTesting2020,
  title = {High-{{Dimensional Independence Testing}} and {{Maximum Marginal Correlation}}},
  author = {Shen, Cencheng},
  year = {2020},
  month = jan,
  abstract = {A number of universally consistent dependence measures have been recently proposed for testing independence, such as distance correlation, kernel correlation, multiscale graph correlation, etc. They provide a satisfactory solution for dependence testing in low-dimensions, but often exhibit decreasing power for high-dimensional data, a phenomenon that has been recognized but remains mostly unchartered. In this paper, we aim to better understand the high-dimensional testing scenarios and explore a procedure that is robust against increasing dimension. To that end, we propose the maximum marginal correlation method and characterize high-dimensional dependence structures via the notion of dependent dimensions. We prove that the maximum method can be valid and universally consistent for testing high-dimensional dependence under regularity conditions, and demonstrate when and how the maximum method may outperform other methods. The methodology can be implemented by most existing dependence measures, has a superior testing power in a variety of common high-dimensional settings, and is computationally efficient for big data analysis when using the distance correlation chi-square test.},
  archiveprefix = {arXiv},
  eprint = {2001.01095},
  eprinttype = {arxiv},
  journal = {arXiv:2001.01095 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {cs, stat}
}

@article{pandaHyppoMultivariateHypothesis2021,
  title = {Hyppo: {{A Multivariate Hypothesis Testing Python Package}}},
  shorttitle = {Hyppo},
  author = {Panda, Sambit and Palaniappan, Satish and Xiong, Junhao and Bridgeford, Eric W. and Mehta, Ronak and Shen, Cencheng and Vogelstein, Joshua T.},
  year = {2021},
  month = apr,
  journal = {arXiv:1907.02088 [cs, stat]},
  eprint = {1907.02088},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce hyppo, a unified library for performing multivariate hypothesis testing, including independence, two-sample, and k-sample testing. While many multivariate independence tests have R packages available, the interfaces are inconsistent and most are not available in Python. hyppo includes many state of the art multivariate testing procedures. The package is easy-to-use and is flexible enough to enable future extensions. The documentation and all releases are available at https://hyppo.neurodata.io.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Mathematical Software,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
}

@article{shenDistanceCorrelationMultiscale2020,
  title = {From {{Distance Correlation}} to {{Multiscale Graph Correlation}}},
  author = {Shen, Cencheng and Priebe, Carey E. and Vogelstein, Joshua T.},
  year = {2020},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {115},
  number = {529},
  pages = {280--291},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2018.1543125},
  abstract = {Understanding and developing a correlation measure that can detect general dependencies is not only imperative to statistics and machine learning, but also crucial to general scientific discovery in the big data age. In this paper, we establish a new framework that generalizes distance correlation (Dcorr)\textemdash a correlation measure that was recently proposed and shown to be universally consistent for dependence testing against all joint distributions of finite moments\textemdash to the multiscale graph correlation (MGC). By using the characteristic functions and incorporating the nearest neighbor machinery, we formalize the population version of local distance correlations, define the optimal scale in a given dependency, and name the optimal local correlation as MGC. The new theoretical framework motivates a theoretically sound sample MGC and allows a number of desirable properties to be proved, including the universal consistency, convergence, and almost unbiasedness of the sample version. The advantages of MGC are illustrated via a comprehensive set of simulations with linear, nonlinear, univariate, multivariate, and noisy dependencies, where it loses almost no power in monotone dependencies while achieving better performance in general dependencies, compared to Dcorr and other popular methods. Supplementary materials for this article are available online.},
  keywords = {Generalized distance correlation,Nearest neighbor graph,Testing independence},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2018.1543125},
}

@article{vogelsteinDiscoveringDecipheringRelationships2019,
  title = {Discovering and Deciphering Relationships across Disparate Data Modalities},
  author = {Vogelstein, Joshua T and Bridgeford, Eric W and Wang, Qing and Priebe, Carey E and Maggioni, Mauro and Shen, Cencheng},
  editor = {Taylor, Dane and Gold, Joshua I},
  year = {2019},
  month = jan,
  journal = {eLife},
  volume = {8},
  pages = {e41690},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.41690},
  abstract = {Understanding the relationships between different properties of data, such as whether a genome or connectome has information about disease status, is increasingly important. While existing approaches can test whether two properties are related, they may require unfeasibly large sample sizes and often are not interpretable. Our approach, `Multiscale Graph Correlation' (MGC), is a dependence test that juxtaposes disparate data science techniques, including k-nearest neighbors, kernel methods, and multiscale analysis. Other methods may require double or triple the number of samples to achieve the same statistical power as MGC in a benchmark suite including high-dimensional and nonlinear relationships, with dimensionality ranging from 1 to 1000. Moreover, MGC uniquely characterizes the latent geometry underlying the relationship, while maintaining computational efficiency. In real data, including brain imaging and cancer genetics, MGC detects the presence of a dependency and provides guidance for the next experiments to conduct.},
  keywords = {data science,machine learning,statistics},
}

@article{escoufierTraitementVariablesVectorielles1973,
  title = {Le {{Traitement}} Des {{Variables Vectorielles}}},
  author = {Escoufier, Yves},
  year = {1973},
  journal = {Biometrics},
  volume = {29},
  number = {4},
  pages = {751--760},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2529140},
  abstract = {Multivariate statistical techniques are based upon the Hilbert space Structure of the set of random variables with finite variance, and are therefore generalizable to any set having such a structure. In Particular, since certain applications lead to families of vector random variables we are led to the problem of finding the Hilbert space which permits the above generalization to families of vector variables. The inner product which is used possesses theoretical and practical properties which are discussed. An illustrative example is given.}
}

@article{robertUnifyingToolLinear1976,
  title = {A {{Unifying Tool}} for {{Linear Multivariate Statistical Methods}}: {{The RV}}- {{Coefficient}}},
  shorttitle = {A {{Unifying Tool}} for {{Linear Multivariate Statistical Methods}}},
  author = {Robert, P. and Escoufier, Y.},
  year = {1976},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {25},
  number = {3},
  pages = {257--265},
  publisher = {{[Wiley, Royal Statistical Society]}},
  issn = {0035-9254},
  doi = {10.2307/2347233},
  abstract = {Consider two data matrices on the same sample of n individuals, X(p \texttimes{} n), Y(q \texttimes{} n). From these matrices, geometrical representations of the sample are obtained as two configurations of n points, in R\textsuperscript{p} and R\textsuperscript{q}. It is shown that the RV-coefficient (Escoufier, 1970, 1973) can be used as a measure of similarity of the two configurations, taking into account the possibly distinct metrics to be used on them to measure the distances between points. The purpose of this paper is to show that most classical methods of linear multivariate statistical analysis can be interpreted as the search for optimal linear transformations or, equivalently, the search for optimal metrics to apply on two data matrices on the same sample; the optimality is defined in terms of the similarity of the corresponing configurations of points. which, in turn, calls for the maximization of the associated RV-coefficient. The methods studied are principal components, principal components of instrumental variables, multivariate regression, canonical variables, discriminant analysis; they are differentiated by the possible relationships existing between the two data matrices involved and by additional constraints under which the maximum of RV is to be obtained. It is also shown that the RV-coefficient can be used as a measure of goodness of a solution to the problem of discarding variables.}
}

@article{pandaNonparMANOVAIndependence2021,
  title = {Nonpar {{MANOVA}} via {{Independence Testing}}},
  author = {Panda, Sambit and Shen, Cencheng and Perry, Ronan and Zorn, Jelle and Lutz, Antoine and Priebe, Carey E. and Vogelstein, Joshua T.},
  year = {2021},
  month = apr,
  journal = {arXiv:1910.08883 [cs, stat]},
  eprint = {1910.08883},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The \$k\$-sample testing problem tests whether or not \$k\$ groups of data points are sampled from the same distribution. Multivariate analysis of variance (MANOVA) is currently the gold standard for \$k\$-sample testing but makes strong, often inappropriate, parametric assumptions. Moreover, independence testing and \$k\$-sample testing are tightly related, and there are many nonparametric multivariate independence tests with strong theoretical and empirical properties, including distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic). We prove that universally consistent independence tests achieve universally consistent \$k\$-sample testing and that \$k\$-sample statistics like Energy and Maximum Mean Discrepancy (MMD) are exactly equivalent to Dcorr. Empirically evaluating these tests for \$k\$-sample scenarios demonstrates that these nonparametric independence tests typically outperform MANOVA, even for Gaussian distributed settings. Finally, we extend these non-parametric \$k\$-sample testing procedures to perform multiway and multilevel tests. Thus, we illustrate the existence of many theoretically motivated and empirically performant \$k\$-sample tests. A Python package with all independence and k-sample tests called hyppo is available from https://hyppo.neurodata.io/.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
}

@article{szekelyTestingEqualDistributions,
  title = {Testing for Equal Distributions in High Dimensions},
  author = {Sz{\'e}kely, G{\'a}bor J. and Rizzo, Maria L.},
  journal = {InterStat},
  year = {},
  pages = {2004},
  abstract = {We propose a new nonparametric test for equality of two or more multivariate distributions based on Euclidean distance between sample elements. Several consistent tests for comparing multivariate distributions can be developed from the underlying theoretical results. The test procedure for the multisample problem is developed and applied for testing the composite hypothesis of equal distributions, when distributions are unspecified. The proposed test is universally consistent against all fixed alternatives (not necessarily continuous) with finite second moments. The test is implemented by conditioning on the pooled sample to obtain an approximate permutation test, which is distribution free. Our Monte Carlo power study suggests that the new test may be much more sensitive than tests based on nearest neighbors against several classes of alternatives, and performs particularly well in high dimension. Computational complexity of our test procedure is},
}

@article{hotellingGeneralizationStudentRatio1931,
  title = {The {{Generalization}} of {{Student}}'s {{Ratio}}},
  author = {Hotelling, Harold},
  year = {1931},
  month = aug,
  journal = {The Annals of Mathematical Statistics},
  volume = {2},
  number = {3},
  pages = {360--378},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177732979},
  abstract = {The Annals of Mathematical Statistics},
}

@article{micceriUnicornNormalCurve1989,
  title = {The Unicorn, the Normal Curve, and Other Improbable Creatures},
  author = {Micceri, Theodore},
  year = {1989},
  journal = {Psychological Bulletin},
  volume = {105},
  number = {1},
  pages = {156--166},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.105.1.156},
  abstract = {An investigation of the distributional characteristics of 440 large-sample achievement and psychometric measures found all to be significantly nonnormal at the alpha .01 significance level. Several classes of contamination were found, including tail weights from the uniform to the double exponential, exponential-level asymmetry, severe digit preferences, multimodalities, and modes external to the mean/median interval. Thus, the underlying tenets of normality-assuming statistics appear fallacious for these commonly used types of data. However, findings here also fail to support the types of distributions used in most prior robustness research suggesting the failure of such statistics under nonnormal conditions. A reevaluation of the statistical robustness literature appears appropriate in light of these findings. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Achievement Measures,Normal Distribution,Psychometrics,Statistical Measurement},
}

@article{stiglerRobustEstimatorsWork1977,
  title = {Do {{Robust Estimators Work}} with {{Real Data}}?},
  author = {Stigler, Stephen M.},
  year = {1977},
  month = nov,
  journal = {The Annals of Statistics},
  volume = {5},
  number = {6},
  pages = {1055--1098},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176343997},
  abstract = {Most studies of robust estimators of location parameters have relied upon mathematical theory, computer simulated data, or a combination of these. This paper presents a comparison of the performances of eleven estimators using real data sets. Twenty sets of data from 1761 determinations of the parallax of the sun, from 1798 measurements of the mean density of the earth, and from circa 1880 measurements of the speed of light, are employed in the study, with the current values of these physical constants being compared with the estimators' realized values. We find that light trimming provides some improvement over the sample mean, but that the sample mean itself compares favorably with many recent proposals. The bias and nonnormality of the data sets is considered, and the data sets are presented and discussed in an appendix.},
  keywords = {$M$-estimators,62-02,62G35,adaptive estimators,bias,kurtosis,median,Monte Carlo,simulation,skewness,Trimmed means},
}

@article{winklerMultilevelBlockPermutation2015,
  title = {Multi-Level Block Permutation},
  author = {Winkler, Anderson M. and Webster, Matthew A. and Vidaurre, Diego and Nichols, Thomas E. and Smith, Stephen M.},
  year = {2015},
  month = dec,
  journal = {NeuroImage},
  volume = {123},
  pages = {253--268},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2015.05.092},
  abstract = {Under weak and reasonable assumptions, mainly that data are exchangeable under the null hypothesis, permutation tests can provide exact control of false positives and allow the use of various non-standard statistics. There are, however, various common examples in which global exchangeability can be violated, including paired tests, tests that involve repeated measurements, tests in which subjects are relatives (members of pedigrees) \textemdash{} any dataset with known dependence among observations. In these cases, some permutations, if performed, would create data that would not possess the original dependence structure, and thus, should not be used to construct the reference (null) distribution. To allow permutation inference in such cases, we test the null hypothesis using only a subset of all otherwise possible permutations, i.e., using only the rearrangements of the data that respect exchangeability, thus retaining the original joint distribution unaltered. In a previous study, we defined exchangeability for blocks of data, as opposed to each datum individually, then allowing permutations to happen within block, or the blocks as a whole to be permuted. Here we extend that notion to allow blocks to be nested, in a hierarchical, multi-level definition. We do not explicitly model the degree of dependence between observations, only the lack of independence; the dependence is implicitly accounted for by the hierarchy and by the permutation scheme. The strategy is compatible with heteroscedasticity and variance groups, and can be used with permutations, sign flippings, or both combined. We evaluate the method for various dependence structures, apply it to real data from the Human Connectome Project (HCP) as an example application, show that false positives can be avoided in such cases, and provide a software implementation of the proposed approach.},
  language = {en},
  keywords = {General linear model,Multiple regression,Permutation inference,Repeated measurements},
}

@article{bartlettNoteTestsSignificance1939,
  title = {A Note on Tests of Significance in Multivariate Analysis},
  author = {Bartlett, M. S.},
  year = {1939},
  month = apr,
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {35},
  number = {2},
  pages = {180--185},
  publisher = {{Cambridge University Press}},
  issn = {1469-8064, 0305-0041},
  doi = {10.1017/S0305004100020880},
  abstract = {Multivariate generalizations. In multivariate statistical analysis, common terms such as variances and correlation coefficients have received certain generalizations. Wilks (7) has called the determinant |V|, where V is the matrix of variances and covariances between several variates, a generalized variance; certain ratios of such determinants have been called by Hotelling(5) vector correlation coefficients and vector alienation coefficients. While these determinantal functions have properties which justify to some extent this kind of generalization, it sometimes seems more reasonable to leave any generalized parameters, or corresponding sample statistics, in the form of matrices of elementary quantities. This is stressed by the formal analogy which then often exists between the generalized and the elementary formulae.},
  language = {en},
}

@article{everittMonteCarloInvestigation1979,
  title = {A {{Monte Carlo Investigation}} of the {{Robustness}} of {{Hotelling}}'s {{One}}- and {{Two}}-{{Sample T2 Tests}}},
  author = {Everitt, Brian S.},
  year = {1979},
  journal = {Journal of the American Statistical Association},
  volume = {74},
  number = {365},
  pages = {48--51},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2286719},
  abstract = {The robustness of Hotelling's one- and two-sample tests is investigated by Monte Carlo methods. Samples are presented from uniform, exponential, and lognormal distributions. The results show that the two-sample test is far more robust to departures from normality than the one-sample test, and that the latter may be badly affected by departures due to extreme skewness.}
}

@article{MultivariateAnalysisVariance,
  title = {Multivariate {{Analysis}} of {{Variance}} ({{MANOVA}})},
  journal = {Multivariate Analysis of Variance},
  pages = {11},
  language = {en},
}

@article{raoTestsSignificanceMultivariate1948,
  title = {Tests of {{Significance}} in {{Multivariate Analysis}}},
  author = {Rao, C. Radhakrishna},
  year = {1948},
  journal = {Biometrika},
  volume = {35},
  number = {1/2},
  pages = {58--79},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332629}
}

@article{warnePrimerMultivariateAnalysis2019,
  title = {A {{Primer}} on {{Multivariate Analysis}} of {{Variance}} ({{MANOVA}}) for {{Behavioral Scientists}}},
  author = {Warne, Russell},
  year = {2019},
  month = nov,
  journal = {Practical Assessment, Research, and Evaluation},
  volume = {19},
  number = {1},
  issn = {1531-7714},
  doi = {10.7275/sm63-7h70},
}

@article{grettonKernelTwoSampleTest2012,
  title = {A {{Kernel Two}}-{{Sample Test}}},
  author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {13},
  number = {25},
  pages = {723--773},
  issn = {1533-7928},
  abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
}

@article{mehtaIndependenceTestingMultivariate2020,
  title = {Independence {{Testing}} for {{Multivariate Time Series}}},
  author = {Mehta, Ronak and Chung, Jaewon and Shen, Cencheng and Xu, Ting and Vogelstein, Joshua T.},
  year = {2020},
  month = may,
  journal = {arXiv:1908.06486 [cs, stat]},
  eprint = {1908.06486},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Complex data structures such as time series are increasingly present in modern data science problems. A fundamental question is whether two such time-series are statistically dependent. Many current approaches make parametric assumptions on the random processes, only detect linear association, require multiple tests, or forfeit power in high-dimensional, nonlinear settings. Estimating the distribution of any test statistic under the null is non-trivial, as the permutation test is invalid. This work juxtaposes distance correlation (Dcorr) and multiscale graph correlation (MGC) from independence testing literature and block permutation from time series analysis to address these challenges. The proposed nonparametric procedure is valid and consistent, building upon prior work by characterizing the geometry of the relationship, estimating the time lag at which dependence is maximized, avoiding the need for multiple testing, and exhibiting superior power in high-dimensional, low sample size, nonlinear settings. Neural connectivity is analyzed via fMRI data, revealing linear dependence of signals within the visual network and default mode network, and nonlinear relationships in other networks. This work uncovers a first-resort data analysis tool with open-source code available, directly impacting a wide range of scientific disciplines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
}

@article{shenChiSquareTestDistance2021,
  title = {The {{Chi}}-{{Square Test}} of {{Distance Correlation}}},
  author = {Shen, Cencheng and Panda, Sambit and Vogelstein, Joshua T.},
  year = {2021},
  month = jun,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {0},
  number = {ja},
  pages = {1--21},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2021.1938585},
  abstract = {Distance correlation has gained much recent attention in the data science community: the sample statistic is straightforward to compute and asymptotically equals zero if and only if independence, making it an ideal choice to discover any type of dependency structure given sufficient sample size. One major bottleneck is the testing process: because the null distribution of distance correlation depends on the underlying random variables and metric choice, it typically requires a permutation test to estimate the null and compute the p-value, which is very costly for large amount of data. To overcome the difficulty, in this paper we propose a chi-square test for distance correlation. Method-wise, the chi-square test is non-parametric, extremely fast, and applicable to bias-corrected distance correlation using any strong negative type metric or characteristic kernel. The test exhibits a similar testing power as the standard permutation test, and can be utilized for K-sample and partial testing. Theory-wise, we show that the underlying chi-square distribution well approximates and dominates the limiting null distribution in upper tail, prove the chi-square test can be valid and universally consistent for testing independence, and establish a testing power inequality with respect to the permutation test.},
  copyright = {All rights reserved},
  keywords = {centered chi-square distribution,nonparametric test,testing independence,unbiased distance covariance},
  annotation = {\_eprint: https://doi.org/10.1080/10618600.2021.1938585},
}

@article{chwialkowski2015fast,
      title={Fast Two-Sample Testing with Analytic Representations of Probability Measures},
      author={Kacper Chwialkowski and Aaditya Ramdas and Dino Sejdinovic and Arthur Gretton},
      year={2015},
      journal={arXiv:1506.04725 [math, stat]},
      print={1506.04725},
      eprinttype={arxiv},
      abstract={We propose a class of nonparametric two-sample tests with a cost linear in the sample size. Two tests are given, both based on an ensemble of distances between analytic functions representing each of the distributions. The first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel Hilbert space. Analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies. The new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distance-based tests. Experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics.},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{grettonKernelJointIndependence2016,
  title = {{Kernel-based Tests} for {Joint Independence}},
  author = {Pfister, Nikolas and Buhlmann, Peter and Scholkopf, Bernhard and Peters, Jonas},
  year = {2016},
  month = nov,
  journal = {arXiv:1603.00285 [math, stat]},
  eprint = {1603.00285},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {We investigate the problem of testing whether d random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two variable Hilbert-Schmidt independence criterion (HSIC) but allows for an arbitrary number of variables. We embed the d-dimensional joint distribution and the product of the marginals into a reproducing kernel Hilbert space and define the d-variable Hilbert-Schmidt independence criterion (dHSIC) as the squared distance between the embeddings. In the population case, the value of dHSIC is zero if and only if the d variables are jointly independent, as long as the kernel is characteristic. Based on an empirical estimate of dHSIC, we define three different non-parametric hypothesis tests: a permutation test, a bootstrap test and a test based on a Gamma approximation. We prove that the permutation test achieves the significance level and that the bootstrap test achieves pointwise asymptotic significance level as well as pointwise asymptotic consistency (i.e., it is able to detect any type of fixed dependence in the large sample limit). The Gamma approximation does not come with these guarantees; however, it is computationally very fast and for small d, it performs well in practice. Finally, we apply the test to a problem in causal discovery.},
  archiveprefix = {arXiv},
  keywords = {Math - Statistics Theory, Statistics - Machine Learning},
 }
 
 @article{friedmanMultivariateGeneralizationsoftheWaldWolfowitzandSmirnovTwoSampleTests1979,
  title = {Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests},
  author = {Friedman, Jerome and Rafsky, Lawrence},
  year = {1979},
  month = jul,
  journal = {Ann. Statist.},
  volume = {7},
  number = {4},
  pages = {697-717},
  doi = {10.1214/aos/1176344722},
  abstract = {Multivariate generalizations of the Wald-Wolfowitz runs statistic and the Smirnov maximum deviation statistic for the two-sample problem are presented. They are based on the minimal spanning tree of the pooled sample points. Some null distribution results are derived and a simulation study of power is reported.},
  copyright = {All rights reserved},
  keywords = {Wald-Wolfowitz Runs Test,randomness,minimum spanning tree},
  annotation = {\_eprint: https://doi.org/10.1214/aos/1176344722},
 }

@inproceedings{hellerMultivariateTestsOfAssociation2016,
  title = {Multivariate Tests of Association Based on Univariate Tests},
  author = {Heller, Ruth and Heller, Yair},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2016/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Paper.pdf},
  volume = {29},
  year = {2016}
}

@misc{sasHoeffdingDependenceCoefficient,
  title = {Hoeffding Dependence Coefficient},
  author = {SAS},
  howpublished = "\url{https://support.sas.com/documentation/cdl/en/procstat/63104/HTML/default/viewer.htm#procstat_corr_sect016.htm}",
  note = {Accessed: 2021-12-17},
}
